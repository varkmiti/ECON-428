{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markviti/opt/anaconda3/envs/data/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.misc import derivative # I found it!\n",
    "import time\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('reg.csv')\n",
    "\n",
    "y = np.array(data['y'])\n",
    "x = np.array(data.iloc[:, 1:])\n",
    "X = x.copy()\n",
    "rows, cols = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "1. We will show show the $L_0$ norm is not convex. Recall that $||B_0||_0 = \\left\\{ i : b_i \\neq 0 \\right\\}$. So, let $x = \\left\\{ 1, 0 \\right\\}$ and $y = \\left\\{ 0, 1 \\right\\}$. Then, $||x||_0 = 1$ and $||y||_0 = 1$. However, $||\\frac{x + y}{2}||_0 = 2$. Now, we have that $||\\frac{1}{2}\\cdot x + \\frac{1}{2}\\cdot y||_0 \\leq \\frac{1}{2}\\cdot||x||_0 + \\frac{1}{2}\\cdot||y||_0$ is the definition of convexity. And so we have that \n",
    "$\\begin{align}\n",
    "||\\frac{1}{2}\\cdot x + \\frac{1}{2}\\cdot y||_0 &\\leq \\frac{1}{2}\\cdot||x||_0 + \\frac{1}{2}\\cdot||y||_0 \\\\\n",
    "2 & \\leq \\frac{1}{2} + \\frac{1}{2} \\\\\n",
    "\\end{align}$\n",
    "Which of course is not true. Thus, the $L_0$ norm is not convex.\n",
    "\n",
    "2. We will use the result from the previous PSET to show that the $L_0$ norm is not a norm. Recall that we showed before that all norms are convex. Since the $L_0$ norm is not convex, it is not a norm.\n",
    "\n",
    "3. We will now show that the $L_1$ norm is convex. To do this, we will show that the $L_1$ norm is a norm, which by the last PSET will imply convexity. \n",
    "\n",
    "    i. We will show that the $L_1$ norm is non-negative. This is clear since the absolute value of any number is non-negative.\n",
    "\n",
    "    ii. We will show that the $L_1$ norm is zero if and only if the vector is the zero vector. This is clear since the absolute value of any number is zero if and only if the number is zero.\n",
    "\n",
    "    iii. We will show that the $L_1$ norm satisfies the triangle inequality. This is clear since the absolute value of the sum of two numbers is less than or equal to the sum of the absolute values of the two numbers. That is $|a + b| \\leq |a| + |b|$. If this is not immediately obvious, note that $|a + b| = |a| + |b|$ if $a$ and $b$ have the same sign, and $|a + b| = |a| - |b|$ if $a$ and $b$ have different signs. In either case, the triangle inequality holds.  \n",
    "    \n",
    "    iv. We will show that the $L_1$ norm is homogenous. This is clear since the absolute value of the product of a number and a vector is equal to the absolute value of the number times the absolute value of the vector.\n",
    "\n",
    "Therefore, the $L_1$ norm is a norm, and thus convex.\n",
    "\n",
    "4. We will now show that the $L_2$ norm is convex. To do this, we will show that the $L_2$ norm is a norm, which by the last PSET will imply convexity. \n",
    "\n",
    "    i. We will show that the $L_2$ norm is non-negative. This is clear since the square of any number is non-negative.\n",
    "\n",
    "    ii. We will show that the $L_2$ norm is zero if and only if the vector is the zero vector. This is clear since the square of any number is zero if and only if the number is zero.\n",
    "\n",
    "    iii. We will show that the $L_2$ norm satisfies the triangle inequality. \n",
    "    $\\begin{align}\n",
    "    ||x + y||_2^2 &= \\sum_{i=1}^{n} (x_i + y_i)^2 \\\\\n",
    "    &= \\sum_{i=1}^{n} x_i^2 + 2x_iy_i + y_i^2 \\\\\n",
    "    &= \\sum_{i=1}^{n} x_i^2 + \\sum_{i=1}^{n} 2x_iy_i + \\sum_{i=1}^{n} y_i^2 \\\\\n",
    "    & \\leq  \\vert\\vert x \\vert\\vert_2^2 + 2\\vert\\vert x \\vert\\vert_2\\vert\\vert y \\vert\\vert_2 + \\vert\\vert y \\vert\\vert_2^2  && \\text{Cauchy-Schwartz}\\\\ \n",
    "    \\implies \\vert\\vert x + y \\vert\\vert_2^2 & \\leq \\left( \\vert\\vert x \\vert\\vert_2 + \\vert\\vert y \\vert\\vert_2 \\right)^2 \\\\\n",
    "    \\end{align}$\n",
    "\n",
    "    iv. We will show that the $L_2$ norm is homogenous. This is clear since the square of the product of a number and a vector is equal to the square of the number times the square of the vector. That is $||\\alpha x||_2 = \\alpha^2||x||_2$.\n",
    "\n",
    "Therefore, the $L_2$ norm is a norm, and thus convex.\n",
    "\n",
    "5. We will now show that $\\alpha ||\\beta||_1 + (1 - \\alpha)||\\beta||_2$ is convex. Note this is very easy. Since the $L_1$ and $L_2$ norms are convex, and since the sum of convex functions is convex (as we proved on the last PSET), we have that $\\alpha ||\\beta||_1 + (1 - \\alpha)||\\beta||_2$ is convex.\n",
    "\n",
    "6. To find out which of these norms results in a convex regression problem for the objective $\\min_{\\beta} \\frac{1}{n}\\sum_{i = 1}^n \\left(y_i - x_i\\beta\\right)^2 + \\lambda h(\\beta)$ is convex. First, now that we have already shown that the first part of the summand is convex. Since the sum of two convex functions is convex (as shown on the last PSET), it must be the case that the entire function is convex for the convex norms. Thus, the $L_1$ and $L_2$ norms result in a convex regression problem. Moreover, the elastic-net problem is also convex. However, the $L_0$ norm does not result in a convex regression problem since this function is not always convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "## Part 1\n",
    "*We do not have any graduate students in the class, so we will not be able to complete this part of the problem.*\n",
    "\n",
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load smoking data - MV\n",
    "\n",
    "smoking = pd.read_csv('soo.csv')\n",
    "smoking = smoking.drop('Unnamed: 0', axis=1)\n",
    "smoking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoker_avg_exyr = np.mean(smoking[smoking['smoking'] == 1]['exyr'])\n",
    "nsmoker_avg_exyr = np.mean(smoking[smoking['smoking'] == 0]['exyr'])\n",
    "print(f'Average life expectancy for smokers: {smoker_avg_exyr}')\n",
    "print(f'Average life expectancy for non-smokers: {nsmoker_avg_exyr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the surface, the claim might appear true since the average years left for smokers is longer than non-smokers. Let's see if that is true when we incorporate data about the current age of the respondents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_smoker_age = np.mean(smoking[smoking['smoking'] == 1]['age'] + smoking[smoking['smoking'] == 1]['exyr'])\n",
    "total_nsmoker_age = np.mean(smoking[smoking['smoking'] == 0]['age'] + smoking[smoking['smoking'] == 0]['exyr'])\n",
    "print(f'Average death age for smokers: {total_smoker_age}')\n",
    "print(f'Average death age for non-smokers: {total_nsmoker_age}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis, we see that smokers tend to live shorter lives than non-smokers. I suspect that the paper's claim is false and that there are a lot of young smokers in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_smoker_age = np.mean(smoking[smoking['smoking'] == 1]['age'])\n",
    "curr_nsmoker_age = np.mean(smoking[smoking['smoking'] == 0]['age'])\n",
    "print(f'Average current age for smokers: {curr_smoker_age}')\n",
    "print(f'Average current age for non-smokers: {curr_nsmoker_age}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that the smokers in this study are younger than the non-smokers, causing the confusion about the total number of years left. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "I will redo what I did for part of part A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_smoker_age = np.mean(smoking[smoking['smoking'] == 1]['age'])\n",
    "curr_nsmoker_age = np.mean(smoking[smoking['smoking'] == 0]['age'])\n",
    "print(f'Average current age for smokers: {curr_smoker_age}')\n",
    "print(f'Average current age for non-smokers: {curr_nsmoker_age}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the smokers being younger on average is causing the problem for causal inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "It might be true that assumption 3 holds. Assumption 3 is known as the conditional independence assumption. This assumption states that the treatment is independent of the potential outcomes given the covariates. In other words, smoking has not been assigned to specific categories of people, once we control for age. This is a strong assumption, and it is not always true. Me might, for example, think that people without access to healthcare might smoke more than those with access to healthcare, which of course would violate the assumption. However, it is certainly possible that this assumption holds (and we have no way to check it with out limited dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "We will code our CATE functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking['smoking_status'] = smoking['smoking'].map({0: 'Non-Smoker', 1: 'Smoker'})\n",
    "\n",
    "f0 = np.polyfit(smoking[smoking['smoking'] == 0]['age'], smoking[smoking['smoking'] == 0]['exyr'], 2)\n",
    "f1 = np.polyfit(smoking[smoking['smoking'] == 1]['age'], smoking[smoking['smoking'] == 1]['exyr'], 2)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='age', y='exyr', data=smoking, hue='smoking_status', palette=['pink', 'lightblue'], alpha=0.75)  \n",
    "\n",
    "# Plot regression lines\n",
    "plt.plot(smoking[smoking['smoking'] == 0]['age'], \n",
    "         np.polyval(f0, smoking[smoking['smoking'] == 0]['age']), \n",
    "         color='blue',  \n",
    "         label='Non-Smoker Fit', linewidth=1)  # Label adjusted for clarity\n",
    "\n",
    "plt.plot(smoking[smoking['smoking'] == 1]['age'], \n",
    "         np.polyval(f1, smoking[smoking['smoking'] == 1]['age']), \n",
    "         color='red',  \n",
    "         label='Smoker Fit', linewidth=1)  # Label adjusted for clarity\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Life Expectancy')\n",
    "plt.title('Life Expectancy vs Age')\n",
    "\n",
    "# Let seaborn handle the legend automatically\n",
    "plt.legend(title='Legend', title_fontsize='13', fontsize='11')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, we can clearly see a treatment affect as the non-smoker data and fit is clearly pushed \"up\" when compared against the smoker data. This means that the non-smokes have a larger intercept than the smokers, meaning they live longer lives. Also not that at any given age, the non-smokers have a greater life expectancy than the smokers, which also shows that we expect non-smokers to be healthier than smokers. That said, we do see the lines begin to converge as age increases (they never actually converge, but the difference between the two lines decreases). This is likely due to two reasons: first, our data just has more older non-smokers, so our fit is better and so this is a natural artifact from our limited smoker data. The second reason is that once someone has lived to a certain age, the effect of smoking on life expectancy is lessened. This is likely because for whatever reason the affects of smoking did not severely impact a person's health. Maybe they avoided cancer for genetic reasons or maybe they were able to quit smoking. However, even for non-smokers the risk of cancer, heart disease, and other diseases increases with age. These factors dominate the affects of smoking for the population who has escaped many of the most severe affects of smoking, causing the affect to decrease with age. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w(a, h, ages):\n",
    "    diffs = a - ages[:, None]  # Compute age differences for all combinations\n",
    "    weights = (1 / (h * np.sqrt(2 * np.pi))) * np.exp(-0.5 * (diffs / h) ** 2)\n",
    "    return weights\n",
    "\n",
    "def ATE(h, smoking):\n",
    "    ages = smoking['age'].values\n",
    "    exyr = smoking['exyr'].values\n",
    "    smoking_status = smoking['smoking'].values\n",
    "    \n",
    "    age_range = np.linspace(ages.min(), ages.max(), len(ages))\n",
    "    \n",
    "    p_a = w(age_range, h, ages)\n",
    "    density = p_a.mean(axis=0)  \n",
    "\n",
    "    treatment_effects = np.polyval(f1, age_range) - np.polyval(f0, age_range)\n",
    "    \n",
    "    ate = np.sum(density * treatment_effects) / len(age_range)\n",
    "    return ate\n",
    "\n",
    "h = 5  \n",
    "ate_var = ATE(h, smoking)\n",
    "print(\"ATE:\", ate_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nsmoker_age + ate_var * total_nsmoker_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our ATE is $-0.05$ and is normalized per year. Thus, we can interpret our ATE as the average number of years lost due to smoking per year. Thus, if we take our average age and multiply it by our ATE, we can get the average number of years lost due to smoking. This number agrees with the average age of death for smokers in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximate the propensity score with degree 3 polynomial\n",
    "def propensity_score(smoking):\n",
    "    model = np.polyfit(smoking['age'], smoking['smoking'], 3)\n",
    "    return np.polyval(model, smoking['age'])\n",
    "\n",
    "# plot the propensity score as a function of age\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(smoking['age'], propensity_score(smoking), color='black', alpha=0.5, linewidths=0.5, edgecolors='k')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Propensity Score')\n",
    "plt.title('Propensity Score vs Age')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo with P instead of X\n",
    "def ATE(h, smoking):\n",
    "    ages = smoking['age'].values\n",
    "    exyr = smoking['exyr'].values\n",
    "    smoking_status = smoking['smoking'].values\n",
    "    p = propensity_score(smoking)\n",
    "    \n",
    "    age_range = np.linspace(ages.min(), ages.max(), len(ages))\n",
    "    \n",
    "    p_a = w(age_range, h, ages)\n",
    "    density = p_a.mean(axis=0)  \n",
    "\n",
    "    treatment_effects = (exyr * smoking_status) / p - (exyr * (1 - smoking_status)) / (1 - p)\n",
    "    \n",
    "    ate = np.sum(density * treatment_effects) / len(age_range)\n",
    "    return ate\n",
    "\n",
    "h = 5\n",
    "ate_var = ATE(h, smoking)\n",
    "print(\"ATE:\", ate_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't care here, but with multiple X values, calculating the kernel for the ATE for each X value could be computationally expensive or confusing. For that reason, using propensity scores to calculate the ATE is a good idea. Also our numbers are very similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_matching(smoking):\n",
    "    # Split into smoker and non-smoker groups\n",
    "    smokers = smoking[smoking['smoking'] == 1].copy()\n",
    "    non_smokers = smoking[smoking['smoking'] == 0].copy()\n",
    "    \n",
    "    # Find nearest non-smoker neighbor for each smoker based on age\n",
    "    smokers['nn'] = smokers['age'].apply(lambda x: (non_smokers['age'] - x).abs().idxmin())\n",
    "    \n",
    "    # Find nearest smoker neighbor for each non-smoker based on age\n",
    "    non_smokers['nn'] = non_smokers['age'].apply(lambda x: (smokers['age'] - x).abs().idxmin())\n",
    "    \n",
    "    return smokers, non_smokers\n",
    "\n",
    "# Matching smokers and non-smokers\n",
    "smokers, non_smokers = nearest_neighbor_matching(smoking)\n",
    "\n",
    "# Calculating CATE for each smoker by comparing with their matched non-smoker\n",
    "smokers['CATE'] = smokers.apply(lambda row: row['exyr'] - non_smokers.loc[non_smokers.index == row['nn'], 'exyr'].values[0], axis=1)\n",
    "\n",
    "# Plotting CATE vs Age for smokers\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(smokers['age'], smokers['CATE'], color='red', alpha=0.5, linewidths=0.5, edgecolors='k')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('CATE')\n",
    "plt.title('CATE vs Age for Smokers')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculating ATE as the mean difference in life expectancy between all smokers and non-smokers\n",
    "ATE = non_smokers['exyr'].mean() - smokers['exyr'].mean() \n",
    "\n",
    "print(\"Total Life ATE:\", ATE)\n",
    "print(\"ATE per Year: \", ATE / total_nsmoker_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our value here is of slightly larger magnitude than the previous value. We already know that our non-smokers tend to be old than our smokers in this dataset (for whatever reason), and so when we do our matching, we are not matching perfectly. That is, we don't always have someone who is a good comparison for any given individual. This is likely causing the discrepancy between the two values. If this is how statistics was supposed to work, then there would be no need for means or regressions or anything. Thus, I am not a fan of this approach. It is a decent approximation, but it is not perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "Find an economics paper that uses selection on observables for which data is available and that you find remotely interesting. Answer the following questions, a few sentences should suffice for each question.\n",
    "\n",
    "### Part A:\n",
    "**List title, author(s), journal, publication date**\n",
    "\n",
    "Title: Estimating the Payoff to Attending a More Selective College: An Application of Selection on Observables and Unobservables\n",
    "Authors: Stacy Berg Dale & Alan B. Krueger\n",
    "Journal: National Bureau of Economic Research\n",
    "Publication Date: August 1999\n",
    "\n",
    "### Part B\n",
    "**What is the causal question they ask, i.e. what is the treatment and what is the outcome?**\n",
    "\n",
    "The causal question investigated in the paper is the effect of attending a more selective college (treatment) on students’ subsequent earnings (outcome).\t\n",
    "\t\n",
    "### Part C\n",
    "**Why does the author/why do the authors expect selection into treatment?**\n",
    "\n",
    "The authors expect selection into treatment due to the admissions process of more selective colleges, which likely consider both observable and unobservable characteristics related to students' future earnings potential. This means students who attend these colleges may already have higher earnings potential before college through characteristics such as motivation, prior academic achievement, and socioeconomic status, leading to a selection bias in estimating the causal effect of college selectivity on earnings.\n",
    "\n",
    "### Part D \n",
    "**What observables are used to control for treatment?**\n",
    "\n",
    "To control for this treatment effect, the study uses observable variables that include students' SAT scores, the average SAT scores of colleges they applied to, and the set of colleges where they were accepted or rejected. These variables help create a more accurate comparison between students who attended more selective colleges and those who did not. The researchers aimed to isolate the effect of college selectivity on future earnings from other factors that could influence earnings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will load and plot the data - MV\n",
    "rd_data = pd.read_csv('rd.csv')\n",
    "rd_data = rd_data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# use seaborn to plot the data - MV\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.scatterplot(x = rd_data['R'], y = rd_data['Y'])\n",
    "ax.set(xlabel='R', ylabel='Outcome: Y')\n",
    "plt.title('Scatterplot of R and Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you can see a discontinuity at 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "I kind of like the idea of running the regression over the entire dataset to either side of the discontinuity. This way, we can see if the regression is different on either side of the discontinuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the regressions on either side of 0.5 - MV\n",
    "\n",
    "# create the dataframes - MV\n",
    "rd_data_1 = rd_data[rd_data['R'] <= 0.5]\n",
    "rd_data_2 = rd_data[rd_data['R'] > 0.5]\n",
    "\n",
    "# run the regressions - MV\n",
    "regression_1 = np.polyfit(rd_data_1['R'], rd_data_1['Y'], 1)\n",
    "regression_2 = np.polyfit(rd_data_2['R'], rd_data_2['Y'], 1)\n",
    "\n",
    "# plot the data and the regressions - MV\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.scatterplot(x = rd_data_1['R'], y = rd_data_1['Y'], color = 'blue')\n",
    "ax = sns.scatterplot(x = rd_data_2['R'], y = rd_data_2['Y'], color = 'orange')\n",
    "ax.set(xlabel='R', ylabel='Outcome: Y')\n",
    "plt.plot(rd_data_1['R'], np.polyval(regression_1, rd_data_1['R']), color = 'red', label = 'Regression 1', linewidth = 2)\n",
    "plt.plot(rd_data_2['R'], np.polyval(regression_2, rd_data_2['R']), color = 'green', label = 'Regression 2', linewidth = 2)\n",
    "plt.title('Scatterplot of R and Y with Regressions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As much as I liked this idea it seemed that Jonas did not like this idea given his response to someone on Ed. Let's try with a buffer of 0.25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_data_25_1 = rd_data[(rd_data['R'] <= 0.5) & (rd_data['R'] > 0.25)]\n",
    "rd_data_25_2 = rd_data[(rd_data['R'] > 0.5) & (rd_data['R'] <= 0.75)]\n",
    "\n",
    "regression_range_1 = np.polyfit(rd_data_25_1['R'], rd_data_25_1['Y'], 1)\n",
    "regression_range_2 = np.polyfit(rd_data_25_2['R'], rd_data_25_2['Y'], 1)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.scatterplot(x='R', y='Y', data=rd_data_25_1, color='blue', label='Data Range 1')\n",
    "ax = sns.scatterplot(x='R', y='Y', data=rd_data_25_2, color='orange', label='Data Range 2')\n",
    "ax.set(xlabel='R', ylabel='Outcome: Y')\n",
    "\n",
    "plt.plot(rd_data_25_1['R'], np.polyval(regression_range_1, rd_data_25_1['R']), color='red', label='Regression Range 1', linewidth=2)\n",
    "plt.plot(rd_data_25_2['R'], np.polyval(regression_range_2, rd_data_25_2['R']), color='green', label='Regression Range 2', linewidth=2)\n",
    "\n",
    "plt.title('Scatterplot of R and Y with Regressions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "To estimate the treatment effect, we will find the jump at 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating the jump\n",
    "jump = (regression_2[1] + regression_2[0] * 0.5) - (regression_1[1] + regression_1[0] * 0.5) # Polyfit returns the slope first - MV\n",
    "print(f'The estimated treatment affect at the cutoff is {jump}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating the jump\n",
    "jump = (regression_range_2[1] + regression_range_2[0] * 0.5) - (regression_range_1[1] + regression_range_1[0] * 0.5) # Polyfit returns the slope first - MV\n",
    "print(f'The estimated treatment affect at the cutoff is {jump}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so, it seems that our affect at 0.5 is somewhere around 0.4. Depending on how large of a buffer we use, we get different results. Note that there is a tradeoff here of more accurate local linear fits and more bias in our estimates. The less data we use, the more bias we have in our estimates, but the more accurate our local linear fits are. Since these predictions are so close, I would be inclined to use the larger buffer because it has more data and is likely to be less biased, but also it likely does not matter since they are close. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "\n",
    "Problem 4.4: Find an economics paper that uses regression discontinuity, for which data is available and that you find remotely interesting. Answer the following questions, a few sentences should suffice for each question.\n",
    "\n",
    "### Part A\n",
    "\n",
    "**List title, author(s), journal, publication date**\n",
    "\n",
    "Title: Measuring the effects of European Regional Policy on economic growth: A regression discontinuity approach\n",
    "Authors: Guido Pellegrini, Flavia Terribile, Ornella Tarola, Teo Muccigrosso, Federic Busillo\n",
    "Journal: Papers in Regional Science\n",
    "Publication Date: March 2013\n",
    "\n",
    "### Part B\n",
    "\n",
    "**What is the causal question they ask, i.e. what is the treatment and what is the outcome?**\n",
    "\n",
    "The causal question is the effect of European Regional Policy on the economic growth of regions  (outcome) that are just below and just above the eligibility threshold for receiving funds (the treatment).\n",
    "\n",
    "### Part C\n",
    "**Why does the author/why do the authors expect selection into treatment?**\n",
    "\n",
    "The authors expect selection into treatment because the allocation of EU Regional Policy funds is based on a cutoff point. That cutoff point states that regions with a per capita GDP below 75% of the EU average are eligible for funds, which introduces a natural selection mechanism for treatment based on economic criteria determined by available data and policy at the time.\n",
    "\n",
    "### Part D\n",
    "\n",
    "**Why does the author/why do the authors expect a discontinuity in treatment assignment?**\n",
    "\n",
    "The authors expect a discontinuity in treatment assignment because the European Regional Policy uses a clear, predefined GDP per capita threshold to determine funding eligibility. Regions just below this cutoff are eligible for assistance, while those just above are not. This creates a natural experiment setting, allowing for the comparison of similar regions on either side of the threshold to estimate the policy's impact on economic growth through a regression discontinuity design analysis, assuming regions near the cutoff are comparable except for their eligibility for funding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
